{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.2178002894356006, 1: 0.041968162083936326, 2: 0.7054992764109985, 3: 0.03473227206946455}\n",
      "Accuracy: 0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/6p9jcp4x46d64zpg9pcffwc40000gn/T/ipykernel_27168/1203289116.py:59: RuntimeWarning: divide by zero encountered in log\n",
      "  score += np.log(feature_probability)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"car_evaluation.csv\")\n",
    "\n",
    "data['decision'] = data['decision'].astype('category').cat.codes\n",
    "X = data.drop(columns=['decision'])\n",
    "y = data['decision']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_probabilities = {}\n",
    "        self.feature_probabilities = {}\n",
    "\n",
    "    def calculate_prior_probabilities(self, y_train):\n",
    "        unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "        total_samples = len(y_train)\n",
    "        self.class_probabilities = dict(zip(unique_classes, class_counts / total_samples))\n",
    "        print(self.class_probabilities)\n",
    "        \n",
    "  \n",
    "\n",
    "    def calculate_feature_probabilities(self, X_train, y_train):\n",
    "        num_samples, num_features = X_train.shape\n",
    "        unique_classes = np.unique(y_train)\n",
    "        self.feature_probabilities = {}\n",
    "\n",
    "        for class_label in unique_classes:\n",
    "            class_indices = np.where(y_train == class_label)\n",
    "            class_samples = X_train.iloc[class_indices]\n",
    "            self.feature_probabilities[class_label] = {}\n",
    "\n",
    "            for feature in X_train.columns:\n",
    "                feature_probabilities = {}\n",
    "                for feature_value in X_train[feature].unique():\n",
    "                    feature_indices = np.where(class_samples[feature] == feature_value)\n",
    "                    feature_probabilities[feature_value] = len(feature_indices[0]) / len(class_indices[0])\n",
    "                self.feature_probabilities[class_label][feature] = feature_probabilities\n",
    "                \n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.calculate_prior_probabilities(y_train)\n",
    "        self.calculate_feature_probabilities(X_train, y_train)\n",
    "        # print(self.calculate_prior_probabilities(y_train))\n",
    "        # print(self.calculate_feature_probabilities(X_train, y_train))\n",
    "\n",
    "    def predict_sample(self, sample):\n",
    "        class_scores = {}\n",
    "        for class_label, class_prob in self.class_probabilities.items():\n",
    "            score = np.log(class_prob)\n",
    "            for feature, feature_value in sample.items():\n",
    "                feature_probability = self.feature_probabilities[class_label][feature].get(feature_value, 1e-6)\n",
    "                score += np.log(feature_probability)\n",
    "            class_scores[class_label] = score\n",
    "        return max(class_scores, key=class_scores.get)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "\n",
    "        for _, sample in X_test.iterrows():\n",
    "            prediction = self.predict_sample(sample)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "# print(feature_probabilities)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Validation Accuracy = 0.5833\n",
      "Fold 2: Validation Accuracy = 0.7917\n",
      "Fold 3: Validation Accuracy = 0.8750\n",
      "Fold 4: Validation Accuracy = 0.9167\n",
      "Fold 5: Validation Accuracy = 0.8333\n",
      "\n",
      "Best Fold: 4\n",
      "Best Validation Accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "column_names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\", \"class\"]\n",
    "data = pd.read_csv(\"iris.csv\", names=column_names)\n",
    "\n",
    "class_mapping = {\"setosa\": 0, \"versicolor\": 1, \"virginica\": 2}\n",
    "data[\"class\"] = data[\"class\"].map(class_mapping)\n",
    "\n",
    "data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "y = data[\"class\"].values\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def initialize_parameters(num_features, num_classes):\n",
    "    W = np.zeros((num_features, num_classes))\n",
    "    b = np.zeros(num_classes)\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def forward_propagation(X, W, b):\n",
    "    Z = np.dot(X, W) + b\n",
    "    A = sigmoid(Z)\n",
    "    return A\n",
    "\n",
    "\n",
    "def compute_cost(Y, A):\n",
    "    m = Y.shape[0]\n",
    "    cost = -np.sum(Y * np.log(A + 1e-10)) / m\n",
    "    return cost\n",
    "\n",
    "\n",
    "def backward_propagation(X, Y, A):\n",
    "    m = X.shape[0]\n",
    "    dZ = A - Y\n",
    "    dW = np.dot(X.T, dZ) / m\n",
    "    db = np.sum(dZ, axis=0) / m\n",
    "    return dW, db\n",
    "\n",
    "\n",
    "def update_parameters(W, b, dW, db, learning_rate):\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    m = len(y)\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    one_hot[np.arange(m), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def train_logistic_regression(X_train, y_train, num_classes, num_iterations, learning_rate):\n",
    "    num_features = X_train.shape[1]\n",
    "    W, b = initialize_parameters(num_features, num_classes)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        A = forward_propagation(X_train, W, b)\n",
    "        cost = compute_cost(one_hot_encode(y_train, num_classes), A)\n",
    "        dW, db = backward_propagation(\n",
    "            X_train, one_hot_encode(y_train, num_classes), A)\n",
    "        W, b = update_parameters(W, b, dW, db, learning_rate)\n",
    "\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def predict(X, W, b):\n",
    "    A = forward_propagation(X, W, b)\n",
    "    return np.argmax(A, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(X, y, k, num_iterations, learning_rate):\n",
    "    num_samples = len(X)\n",
    "    fold_size = num_samples // k\n",
    "    best_accuracy = 0.0\n",
    "    best_fold = None\n",
    "\n",
    "    for fold in range(k):\n",
    "        start = fold * fold_size\n",
    "        end = (fold + 1) * fold_size\n",
    "        X_valid_fold, y_valid_fold = X[start:end], y[start:end]\n",
    "        X_train_fold = np.concatenate([X[:start], X[end:]])\n",
    "        y_train_fold = np.concatenate([y[:start], y[end:]])\n",
    "\n",
    "        W, b = train_logistic_regression(X_train_fold, y_train_fold, num_classes, num_iterations, learning_rate)\n",
    "        y_pred_fold = predict(X_valid_fold, W, b)\n",
    "        accuracy = np.mean(y_pred_fold == y_valid_fold)\n",
    "\n",
    "        print(f\"Fold {fold + 1}: Validation Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_fold = fold\n",
    "\n",
    "    return best_fold, best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "num_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "k_folds = 5\n",
    "\n",
    "best_fold, best_accuracy = k_fold_cross_validation(X_train, y_train, k_folds, num_iterations, learning_rate)\n",
    "\n",
    "print(f\"\\nBest Fold: {best_fold + 1}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
